import os
import json
import csv
import base64
import requests
from openai import OpenAI

# Initialize OpenAI client with API key
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Max tokens for GPT-4o-mini
MAX_TOKENS = 3000
GPT_4O_MODEL = "gpt-4o-mini"

def summarize_google_takeout_llm(takeout_folder, summary_file="llm_takeout_summary.txt"):
    """
    Summarizes Google Takeout data by analyzing text and image files using GPT-4o-mini.
    
    Args:
        takeout_folder (str): The root directory of the Google Takeout data.
        summary_file (str): The output file to write summaries to.
    """
    image_summaries = []

    print(f"Starting to summarize Google Takeout data from: {takeout_folder}")
    with open(summary_file, "w") as summary:
        summary.write(f"Google Takeout Summary for folder: {takeout_folder}\n\n")
        
        for root, dirs, files in os.walk(takeout_folder):
            print(f"Processing directory: {root} with {len(files)} files")
            for file_name in files:
                file_path = os.path.join(root, file_name)
                print(f"Processing file: {file_path}")
                
                if file_path.endswith((".jpg", ".jpeg", ".png", ".gif", ".webp")):
                    try:
                        image_summary = process_image(file_path)
                        image_summaries.append(f"{file_name}: {image_summary}")
                    except Exception as e:
                        print(f"Error processing image {file_path}: {e}")
                else:
                    file_content = process_text_file(file_path)
                    if file_content:
                        # Pass the summary file to write chunk summaries directly
                        generate_summary_with_llm(file_content, summary)

        # Add image summaries at the end
        if image_summaries:
            summary.write("Image Summaries:\n")
            for img_summary in image_summaries:
                summary.write(f"- {img_summary}\n")

        summary.write("\nLLM Summary completed.\n")
    print("Summarization process completed.")


def process_text_file(file_path):
    """
    Reads and returns the content of a file based on its type.
    
    Args:
        file_path (str): The path to the file to be processed.
        
    Returns:
        str: The content of the file as a string.
    """
    if file_path.endswith(".mbox"):
        return summarize_mbox(file_path)
    elif file_path.endswith((".json", ".csv", ".txt")):
        return read_text_based_file(file_path)
    else:
        return None


def summarize_mbox(file_path):
    """
    Summarizes an mbox file by extracting email metadata.

    Args:
        file_path (str): The path to the mbox file.

    Returns:
        str: A summary of email metadata from the mbox file.
    """
    from mailbox import mbox
    mbox_obj = mbox(file_path)
    email_summaries = [f"From: {email['From']} - Subject: {email['Subject']}" for email in mbox_obj]
    return "\n".join(email_summaries[:10])  # Limit to first 10 emails


def read_text_based_file(file_path):
    """
    Reads the content of a text-based file such as JSON, CSV, or plain text.

    Args:
        file_path (str): The path to the file.

    Returns:
        str: The content of the file as a string.
    """
    if file_path.endswith(".json"):
        return json.dumps(load_json(file_path), indent=2)
    elif file_path.endswith(".csv"):
        return process_csv(file_path)
    else:
        with open(file_path, 'r') as file:
            return file.read()


def load_json(file_path):
    """
    Loads and returns data from a JSON file.

    Args:
        file_path (str): The path to the JSON file.

    Returns:
        dict: The loaded JSON data.
    """
    print(f"Loading JSON file: {file_path}")
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except json.JSONDecodeError:
        print(f"Error: Failed to decode JSON from {file_path}.")
        return {}


def process_csv(file_path):
    """
    Processes and returns the content of a CSV file as a string.

    Args:
        file_path (str): The path to the CSV file.

    Returns:
        str: The first 10 rows of the CSV file as a string.
    """
    rows = []
    with open(file_path, 'r') as csvfile:
        reader = csv.reader(csvfile)
        for row in reader:
            rows.append(", ".join(row))
    return "\n".join(rows[:10])  # Limit to first 10 rows


def process_image(image_path):
    """
    Encodes an image in base64 and sends it to GPT-4o-mini for summarization.

    Args:
        image_path (str): The path to the image file.

    Returns:
        str: The image summary generated by GPT-4o-mini.
    """
    print(f"Encoding image: {image_path}")
    base64_image = encode_image(image_path)
    
    # Prepare request for GPT-4o-mini with vision
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {client.api_key}"
    }
    payload = {
        "model": GPT_4O_MODEL,
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Whatâ€™s in this image?"},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }
                    }
                ]
            }
        ],
        "max_tokens": 300
    }
    
    # Send request to OpenAI API
    response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload)
    
    if response.status_code != 200:
        print(f"Error: Failed to process image {image_path}. API response: {response.text}")
        return "Failed to process image"
    
    try:
        return response.json()["choices"][0]["message"]["content"]
    except KeyError:
        print(f"Error: Missing 'choices' key in API response for image {image_path}.")
        return "No summary available for image"


def encode_image(image_path):
    """
    Encodes an image file as a base64 string.

    Args:
        image_path (str): The path to the image file.

    Returns:
        str: The base64 encoded image string.
    """
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')


def generate_summary_with_llm(accumulated_text, summary_file):
    """
    Sends chunks of accumulated text to GPT-4o-mini for summarization and writes the summaries to a file.

    Args:
        accumulated_text (str): The text data to be summarized.
        summary_file (file object): The file to write the summaries to.
    """
    chunk_size = 10000  # Define a chunk size below the token limit
    chunks = [accumulated_text[i:i+chunk_size] for i in range(0, len(accumulated_text), chunk_size)]
    
    for chunk in chunks:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Summarize the following Google Takeout data only in bullet points no sentences to capture user details specifically."},
                {"role": "user", "content": chunk}
            ]
        )
        chunk_summary = response.choices[0].message.content
        print(f"Chunk summary: {chunk_summary}")
        
        # Write the chunk summary to the file immediately
        summary_file.write(f"{chunk_summary}\n\n")


# Main script execution
if __name__ == "__main__":
    takeout_directory = input("Enter the path to your Google Takeout folder: ")
    summarize_google_takeout_llm(takeout_directory)
